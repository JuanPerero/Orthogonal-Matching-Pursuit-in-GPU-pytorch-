{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f9b8324",
   "metadata": {},
   "source": [
    "# Benchmark de batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a0a62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from ompingpu import omp_batch\n",
    "import gc\n",
    "\n",
    "def benchmark_omp_batch():\n",
    "    \"\"\"\n",
    "    EvalÃºa mÃ©tricas especÃ­ficas de omp_batch:\n",
    "    - Memoria mÃ¡xima ocupada\n",
    "    - Tiempo por batch procesado\n",
    "    - Tiempo en procesar dataset completo\n",
    "    - Memoria total ocupada en cada procesamiento\n",
    "    \"\"\"\n",
    "    \n",
    "    # ConfiguraciÃ³n del benchmark\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Usando device: {device}\")\n",
    "    \n",
    "    # ParÃ¡metros fijos del problema\n",
    "    n_samples = 10000      # NÃºmero total de seÃ±ales\n",
    "    signal_length = 512    # Longitud de cada seÃ±al\n",
    "    dict_size = 1024       # TamaÃ±o del diccionario\n",
    "    n_nonzero_coefs = 10   # Coeficientes no ceros\n",
    "    \n",
    "    # TamaÃ±os de batch a evaluar\n",
    "    batch_sizes = [50, 100, 250, 500, 750, 1000, 1500, 2000, 2500, 5000]\n",
    "    \n",
    "    # Generar datos sintÃ©ticos\n",
    "    print(\"Generando datos sintÃ©ticos...\")\n",
    "    torch.manual_seed(42)  # Para reproducibilidad\n",
    "    \n",
    "    # Diccionario normalizado\n",
    "    X = torch.randn(dict_size, signal_length, device=device, dtype=torch.float32)\n",
    "    X = X / torch.norm(X, dim=1, keepdim=True)  # Normalizar columnas\n",
    "    \n",
    "    # SeÃ±ales sintÃ©ticas (esparse)\n",
    "    Y = torch.randn(n_samples, signal_length, device=device, dtype=torch.float32)\n",
    "    \n",
    "    print(f\"Dimensiones: X={X.shape}, Y={Y.shape}\")\n",
    "    \n",
    "    # Calentamiento de GPU\n",
    "    if device == 'cuda':\n",
    "        print(\"Calentando GPU...\")\n",
    "        _ = omp_batch(X, Y[:100], n_nonzero_coefs, batch_size=50)\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Almacenar resultados\n",
    "    results = []\n",
    "    \n",
    "    print(\"\\nIniciando benchmark...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"Evaluando batch_size = {batch_size}\")\n",
    "        \n",
    "        # MÃ©tricas por ejecuciÃ³n\n",
    "        dataset_times = []\n",
    "        max_memories = []\n",
    "        total_memories = []\n",
    "        batch_times = []\n",
    "        \n",
    "        for run in range(3):  # 3 repeticiones\n",
    "            if device == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "                torch.cuda.synchronize()\n",
    "                baseline_memory = torch.cuda.memory_allocated()\n",
    "            \n",
    "            # Calcular nÃºmero de batches\n",
    "            num_batches = (n_samples + batch_size - 1) // batch_size\n",
    "            \n",
    "            # Medir tiempo total del dataset\n",
    "            dataset_start = time.time()\n",
    "            \n",
    "            # Simular procesamiento por batches para medir tiempo individual\n",
    "            individual_batch_times = []\n",
    "            \n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                end_idx = min(i + batch_size, n_samples)\n",
    "                batch_data = Y[i:end_idx]\n",
    "                \n",
    "                batch_start = time.time()\n",
    "                \n",
    "                # Ejecutar OMP para este batch\n",
    "                result_batch = omp_batch(X, batch_data, n_nonzero_coefs, batch_size=batch_size)\n",
    "                \n",
    "                if device == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "                \n",
    "                batch_end = time.time()\n",
    "                individual_batch_times.append(batch_end - batch_start)\n",
    "                \n",
    "                del result_batch\n",
    "            \n",
    "            dataset_end = time.time()\n",
    "            \n",
    "            # MÃ©tricas de tiempo\n",
    "            total_dataset_time = dataset_end - dataset_start\n",
    "            avg_batch_time = np.mean(individual_batch_times)\n",
    "            \n",
    "            dataset_times.append(total_dataset_time)\n",
    "            batch_times.append(avg_batch_time)\n",
    "            \n",
    "            # MÃ©tricas de memoria\n",
    "            if device == 'cuda':\n",
    "                peak_memory = torch.cuda.max_memory_allocated()\n",
    "                current_memory = torch.cuda.memory_allocated()\n",
    "                \n",
    "                max_memory_used = (peak_memory - baseline_memory) / 1024**2  # MB\n",
    "                total_memory_used = (current_memory - baseline_memory) / 1024**2  # MB\n",
    "                \n",
    "                max_memories.append(max_memory_used)\n",
    "                total_memories.append(total_memory_used)\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            gc.collect()\n",
    "        \n",
    "        # EstadÃ­sticas finales\n",
    "        avg_dataset_time = np.mean(dataset_times)\n",
    "        std_dataset_time = np.std(dataset_times)\n",
    "        avg_batch_time = np.mean(batch_times)\n",
    "        std_batch_time = np.std(batch_times)\n",
    "        \n",
    "        if device == 'cuda':\n",
    "            avg_max_memory = np.mean(max_memories)\n",
    "            std_max_memory = np.std(max_memories)\n",
    "            avg_total_memory = np.mean(total_memories)\n",
    "            std_total_memory = np.std(total_memories)\n",
    "        else:\n",
    "            avg_max_memory = avg_total_memory = 0\n",
    "            std_max_memory = std_total_memory = 0\n",
    "        \n",
    "        # MÃ©tricas derivadas\n",
    "        throughput = n_samples / avg_dataset_time\n",
    "        batches_per_second = (n_samples / batch_size) / avg_dataset_time\n",
    "        \n",
    "        result = {\n",
    "            'batch_size': batch_size,\n",
    "            'num_batches': (n_samples + batch_size - 1) // batch_size,\n",
    "            \n",
    "            # Tiempo por batch procesado\n",
    "            'avg_batch_time': avg_batch_time,\n",
    "            'std_batch_time': std_batch_time,\n",
    "            'batch_time_ms': avg_batch_time * 1000,\n",
    "            \n",
    "            # Tiempo dataset completo\n",
    "            'avg_dataset_time': avg_dataset_time,\n",
    "            'std_dataset_time': std_dataset_time,\n",
    "            \n",
    "            # Memoria mÃ¡xima ocupada\n",
    "            'max_memory_mb': avg_max_memory,\n",
    "            'std_max_memory_mb': std_max_memory,\n",
    "            \n",
    "            # Memoria total ocupada\n",
    "            'total_memory_mb': avg_total_memory,\n",
    "            'std_total_memory_mb': std_total_memory,\n",
    "            \n",
    "            # MÃ©tricas adicionales\n",
    "            'throughput_samples_per_sec': throughput,\n",
    "            'batches_per_second': batches_per_second,\n",
    "            'memory_per_sample_kb': (avg_max_memory * 1024) / batch_size if avg_max_memory > 0 else 0\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"  ðŸ“Š RESULTADOS:\")\n",
    "        print(f\"    Dataset completo: {avg_dataset_time:.3f}Â±{std_dataset_time:.3f}s\")\n",
    "        print(f\"    Tiempo por batch: {avg_batch_time*1000:.2f}Â±{std_batch_time*1000:.2f}ms\")\n",
    "        print(f\"    Memoria mÃ¡xima: {avg_max_memory:.1f}Â±{std_max_memory:.1f} MB\")\n",
    "        print(f\"    Memoria total: {avg_total_memory:.1f}Â±{std_total_memory:.1f} MB\")\n",
    "        print(f\"    Throughput: {throughput:.1f} muestras/s\")\n",
    "        print()\n",
    "    \n",
    "    return results, {'n_samples': n_samples, 'signal_length': signal_length, \n",
    "                    'dict_size': dict_size, 'n_nonzero_coefs': n_nonzero_coefs}\n",
    "\n",
    "def plot_results(results, config):\n",
    "    \"\"\"Grafica los resultados del benchmark enfocado en las mÃ©tricas especÃ­ficas\"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(f'OMP Batch Benchmark Detallado - {config[\"n_samples\"]:,} muestras', fontsize=14)\n",
    "    \n",
    "    # 1. Tiempo por batch procesado\n",
    "    ax1.errorbar(df['batch_size'], df['batch_time_ms'], yerr=df['std_batch_time']*1000, \n",
    "                marker='o', capsize=5, capthick=2, color='blue')\n",
    "    ax1.set_xlabel('Batch Size')\n",
    "    ax1.set_ylabel('Tiempo por Batch (ms)')\n",
    "    ax1.set_title('Tiempo por Batch Procesado')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # 2. Tiempo dataset completo\n",
    "    ax2.errorbar(df['batch_size'], df['avg_dataset_time'], yerr=df['std_dataset_time'], \n",
    "                marker='s', capsize=5, capthick=2, color='green')\n",
    "    ax2.set_xlabel('Batch Size')\n",
    "    ax2.set_ylabel('Tiempo Dataset Completo (s)')\n",
    "    ax2.set_title('Tiempo Procesamiento Dataset Completo')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xscale('log')\n",
    "    \n",
    "    # 3. Memoria mÃ¡xima ocupada\n",
    "    if df['max_memory_mb'].sum() > 0:\n",
    "        ax3.errorbar(df['batch_size'], df['max_memory_mb'], yerr=df['std_max_memory_mb'], \n",
    "                    marker='^', capsize=5, capthick=2, color='red')\n",
    "        ax3.set_xlabel('Batch Size')\n",
    "        ax3.set_ylabel('Memoria MÃ¡xima (MB)')\n",
    "        ax3.set_title('Memoria MÃ¡xima Ocupada')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        ax3.set_xscale('log')\n",
    "        ax3.set_yscale('log')\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, 'Memoria no disponible\\n(CPU mode)', \n",
    "                ha='center', va='center', transform=ax3.transAxes)\n",
    "        ax3.set_title('Memoria MÃ¡xima Ocupada')\n",
    "    \n",
    "    # 4. Memoria total ocupada\n",
    "    if df['total_memory_mb'].sum() > 0:\n",
    "        ax4.errorbar(df['batch_size'], df['total_memory_mb'], yerr=df['std_total_memory_mb'], \n",
    "                    marker='d', capsize=5, capthick=2, color='purple')\n",
    "        ax4.set_xlabel('Batch Size')\n",
    "        ax4.set_ylabel('Memoria Total (MB)')\n",
    "        ax4.set_title('Memoria Total Ocupada')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        ax4.set_xscale('log')\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'Memoria no disponible\\n(CPU mode)', \n",
    "                ha='center', va='center', transform=ax4.transAxes)\n",
    "        ax4.set_title('Memoria Total Ocupada')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def print_summary(results, config):\n",
    "    \"\"\"Imprime un resumen detallado de las mÃ©tricas especÃ­ficas\"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"RESUMEN DETALLADO DEL BENCHMARK\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"ConfiguraciÃ³n:\")\n",
    "    print(f\"  - Muestras totales: {config['n_samples']:,}\")\n",
    "    print(f\"  - Longitud seÃ±al: {config['signal_length']}\")\n",
    "    print(f\"  - TamaÃ±o diccionario: {config['dict_size']}\")\n",
    "    print(f\"  - Coeficientes no-cero: {config['n_nonzero_coefs']}\")\n",
    "    print()\n",
    "    \n",
    "    # ANÃLISIS DE TIEMPO POR BATCH\n",
    "    print(\"ðŸ“Š ANÃLISIS DE TIEMPO POR BATCH:\")\n",
    "    fastest_batch_idx = df['avg_batch_time'].idxmin()\n",
    "    slowest_batch_idx = df['avg_batch_time'].idxmax()\n",
    "    \n",
    "    print(f\"  Batch mÃ¡s rÃ¡pido: {df.iloc[fastest_batch_idx]['batch_size']} \"\n",
    "          f\"({df.iloc[fastest_batch_idx]['batch_time_ms']:.2f} ms/batch)\")\n",
    "    print(f\"  Batch mÃ¡s lento: {df.iloc[slowest_batch_idx]['batch_size']} \"\n",
    "          f\"({df.iloc[slowest_batch_idx]['batch_time_ms']:.2f} ms/batch)\")\n",
    "    print()\n",
    "    \n",
    "    # ANÃLISIS DE DATASET COMPLETO\n",
    "    print(\"ðŸ“Š ANÃLISIS DE DATASET COMPLETO:\")\n",
    "    fastest_dataset_idx = df['avg_dataset_time'].idxmin()\n",
    "    slowest_dataset_idx = df['avg_dataset_time'].idxmax()\n",
    "    \n",
    "    print(f\"  Procesamiento mÃ¡s rÃ¡pido: batch_size={df.iloc[fastest_dataset_idx]['batch_size']} \"\n",
    "          f\"({df.iloc[fastest_dataset_idx]['avg_dataset_time']:.2f}s)\")\n",
    "    print(f\"  Procesamiento mÃ¡s lento: batch_size={df.iloc[slowest_dataset_idx]['batch_size']} \"\n",
    "          f\"({df.iloc[slowest_dataset_idx]['avg_dataset_time']:.2f}s)\")\n",
    "    print(f\"  Mejor throughput: {df.iloc[fastest_dataset_idx]['throughput_samples_per_sec']:.1f} muestras/s\")\n",
    "    print()\n",
    "    \n",
    "    # ANÃLISIS DE MEMORIA\n",
    "    if df['max_memory_mb'].sum() > 0:\n",
    "        print(\"ðŸ“Š ANÃLISIS DE MEMORIA:\")\n",
    "        min_memory_idx = df['max_memory_mb'].idxmin()\n",
    "        max_memory_idx = df['max_memory_mb'].idxmax()\n",
    "        \n",
    "        print(f\"  Menor memoria mÃ¡xima: batch_size={df.iloc[min_memory_idx]['batch_size']} \"\n",
    "              f\"({df.iloc[min_memory_idx]['max_memory_mb']:.1f} MB)\")\n",
    "        print(f\"  Mayor memoria mÃ¡xima: batch_size={df.iloc[max_memory_idx]['batch_size']} \"\n",
    "              f\"({df.iloc[max_memory_idx]['max_memory_mb']:.1f} MB)\")\n",
    "        \n",
    "        # Eficiencia de memoria\n",
    "        df['memory_efficiency'] = df['batch_size'] / df['max_memory_mb']\n",
    "        most_efficient_idx = df['memory_efficiency'].idxmax()\n",
    "        print(f\"  MÃ¡s eficiente memoria: batch_size={df.iloc[most_efficient_idx]['batch_size']} \"\n",
    "              f\"({df.iloc[most_efficient_idx]['memory_per_sample_kb']:.2f} KB/muestra)\")\n",
    "        print()\n",
    "    \n",
    "    # RECOMENDACIONES\n",
    "    print(\"ðŸŽ¯ RECOMENDACIONES:\")\n",
    "    \n",
    "    # Encontrar el batch size Ã³ptimo (balance tiempo-memoria)\n",
    "    if df['max_memory_mb'].sum() > 0:\n",
    "        # Normalizar mÃ©tricas para encontrar balance\n",
    "        df_norm = df.copy()\n",
    "        df_norm['norm_time'] = (df_norm['avg_dataset_time'] - df_norm['avg_dataset_time'].min()) / (df_norm['avg_dataset_time'].max() - df_norm['avg_dataset_time'].min())\n",
    "        df_norm['norm_memory'] = (df_norm['max_memory_mb'] - df_norm['max_memory_mb'].min()) / (df_norm['max_memory_mb'].max() - df_norm['max_memory_mb'].min())\n",
    "        df_norm['balance_score'] = df_norm['norm_time'] + df_norm['norm_memory']\n",
    "        \n",
    "        optimal_idx = df_norm['balance_score'].idxmin()\n",
    "        optimal_batch = df.iloc[optimal_idx]['batch_size']\n",
    "        \n",
    "        print(f\"  Batch size Ã³ptimo (balance tiempo-memoria): {optimal_batch}\")\n",
    "        print(f\"    - Tiempo dataset: {df.iloc[optimal_idx]['avg_dataset_time']:.2f}s\")\n",
    "        print(f\"    - Memoria mÃ¡xima: {df.iloc[optimal_idx]['max_memory_mb']:.1f} MB\")\n",
    "        print(f\"    - Throughput: {df.iloc[optimal_idx]['throughput_samples_per_sec']:.1f} muestras/s\")\n",
    "    else:\n",
    "        print(f\"  Batch size Ã³ptimo (menor tiempo): {df.iloc[fastest_dataset_idx]['batch_size']}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # TABLA COMPLETA DE RESULTADOS\n",
    "    print(\"ðŸ“‹ TABLA DETALLADA DE RESULTADOS:\")\n",
    "    print(\"-\" * 120)\n",
    "    \n",
    "    # Seleccionar columnas importantes para mostrar\n",
    "    display_cols = ['batch_size', 'num_batches', 'batch_time_ms', 'avg_dataset_time', \n",
    "                   'max_memory_mb', 'total_memory_mb', 'throughput_samples_per_sec']\n",
    "    \n",
    "    display_df = df[display_cols].copy()\n",
    "    display_df.columns = ['Batch Size', 'Num Batches', 'Tiempo/Batch (ms)', \n",
    "                         'Tiempo Dataset (s)', 'Mem Max (MB)', 'Mem Total (MB)', 'Throughput (muestras/s)']\n",
    "    \n",
    "    print(display_df.to_string(index=False, float_format='%.2f'))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# FunciÃ³n principal para ejecutar todo\n",
    "def run_benchmark():\n",
    "    \"\"\"Ejecuta el benchmark completo\"\"\"\n",
    "    \n",
    "    print(\"ðŸš€ Iniciando Benchmark OMP Batch\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Ejecutar benchmark\n",
    "    results, config = benchmark_omp_batch()\n",
    "    \n",
    "    # Mostrar resumen\n",
    "    df = print_summary(results, config)\n",
    "    \n",
    "    # Graficar resultados\n",
    "    fig = plot_results(results, config)\n",
    "    \n",
    "    # Guardar resultados\n",
    "    df.to_csv('omp_batch_benchmark.csv', index=False)\n",
    "    fig.savefig('omp_batch_benchmark.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Resultados guardados en:\")\n",
    "    print(f\"  - omp_batch_benchmark.csv\")\n",
    "    print(f\"  - omp_batch_benchmark.png\")\n",
    "    \n",
    "    return results, config\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results, config = run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d3d0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
